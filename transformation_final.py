# -*- coding: utf-8 -*-
"""Load.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qev8WjqG-NUGtl4kfPzIwJGoF6tn72YN

### import libraries and packages
"""

import boto3
import pandas as pd
import io
from dotenv import load_dotenv
import os

print('libraries and packages imported')

"""### Read processed csv files from S3 buckets"""

load_dotenv('processed.env')  # Load the environmental file
access_key_id = os.getenv('P_API_KEY')
secret_access_key = os.getenv('P_SECRET_KEY')

# s3 credentials
s3 = boto3.resource(
    service_name='s3',
    region_name='us-east-2',
    aws_access_key_id=access_key_id,
    aws_secret_access_key=secret_access_key
)

# initialize variables
target_bucket_name = 'ds4a-ppp-loan-transformed'
source_bucket_name = 'ds4a-ppp-loan-processed'

# create functions for downloading/uploading
def download_file_from_s3(key, filename):
  s3.Bucket(source_bucket_name).download_file(Key=key, Filename=filename)

# instead of saving intermediate CSV files to disk and then uploading them to S3,
# this function writes the data directly to memory and uploads it to S3 using the
# put_object() method; this eliminates the need for disk I/O operations

def upload_file_to_s3(df_name, filename):
  csv_buffer = io.StringIO()
  df_name.to_csv(csv_buffer, index=False)
  s3.Object(target_bucket_name, filename).put(Body=csv_buffer.getvalue())
  print(filename + ' uploaded to s3')

"""### create dim_naics"""

print('starting transformation of NAICS data')

# download processed NAICS data
download_file_from_s3('source2/pnaics.csv', 'pnaics.csv')
pd_naics = pd.read_csv('pnaics.csv')

# transform NAICS data
pd_naics['2017 NAICS Code'] = pd_naics['2017 NAICS Code'].astype(int)

# Upload dim_naics directly to s3
upload_file_to_s3(pd_naics, 'dim_naics.csv')

"""### create dim_borrower"""

print('starting transformation of borrower data')

# Download and process dim_borrower
download_file_from_s3('source1/ploan_sub.csv', 'ploan_sub.csv')
pd_loans = pd.read_csv('ploan_sub.csv')
pd_borrower = pd_loans[['BorrowerID', 'BorrowerName', 'BorrowerAddress', 'BorrowerCity', 'BorrowerState', 'BorrowerZip', 'Latitude', 'Longitude', 'County']].copy()

# Upload dim_borrower directly
upload_file_to_s3(pd_borrower, 'dim_borrower.csv')

"""### create dim_lender"""

print('starting transformation of lender data')

# Download plender - no processing needed
download_file_from_s3('source1/plender.csv', 'plender.csv')
pd_lender = pd.read_csv('plender.csv')

# upload dim_lender
upload_file_to_s3(pd_lender, 'dim_lender.csv')

"""### create dim_balancesheets
foreign keys are lenderID
"""

print('starting transformation of balance sheets')

# download and process balance sheets
download_file_from_s3('source3/pbalance_sheet.csv', 'pbalance_sheet.csv')
pd_balancesheets = pd.read_csv('pbalance_sheet.csv')

pd_balancesheets = pd_balancesheets.drop(['asOfDate', 'CashAndCashEquivalents'], axis=1)
pd_balancesheets['CommonStockEquity'] = pd_balancesheets['CommonStockEquity'].astype("Int64")
pd_balancesheets['InvestedCapital'] = pd_balancesheets['InvestedCapital'].astype("Int64")
pd_balancesheets['NetDebt'] = pd_balancesheets['NetDebt'].astype("Int64")
pd_balancesheets['NetTangibleAssets'] = pd_balancesheets['NetTangibleAssets'].astype("Int64")
pd_balancesheets['OrdinarySharesNumber'] = pd_balancesheets['OrdinarySharesNumber'].astype("Int64")
pd_balancesheets['PreferredSharesNumber'] = pd_balancesheets['PreferredSharesNumber'].astype("Int64")
pd_balancesheets['PreferredStockEquity'] = pd_balancesheets['PreferredStockEquity'].astype("Int64")
pd_balancesheets['ShareIssued'] = pd_balancesheets['ShareIssued'].astype("Int64")
pd_balancesheets['TangibleBookValue'] = pd_balancesheets['TangibleBookValue'].astype("Int64")
pd_balancesheets['TotalAssets'] = pd_balancesheets['TotalAssets'].astype("Int64")
pd_balancesheets['TotalCapitalization'] = pd_balancesheets['TotalCapitalization'].astype("Int64")
pd_balancesheets['TotalDebt'] = pd_balancesheets['TotalDebt'].astype("Int64")
pd_balancesheets['TotalEquityGrossMinorityInterest'] = pd_balancesheets['TotalEquityGrossMinorityInterest'].astype("Int64")
pd_balancesheets['TotalLiabilitiesNetMinorityInterest'] = pd_balancesheets['TotalLiabilitiesNetMinorityInterest'].astype("Int64")
pd_balancesheets['TreasurySharesNumber'] = pd_balancesheets['TreasurySharesNumber'].astype("Int64")

# upload dim_balancesheets
upload_file_to_s3(pd_balancesheets, 'dim_balancesheets.csv')

"""### create fact_stocks"""

print('starting transformation of stocks')

# download and process stock data
download_file_from_s3('source4/pstock.csv', 'pstock.csv')
pd_stocks = pd.read_csv('pstock.csv')

#temporarily dropping the additional BorrowerID column - will remove this
pd_stocks = pd_stocks.drop('BorrowerID', axis=1)

# upload fact_stocks
upload_file_to_s3(pd_stocks, 'fact_stocks.csv')

"""### create fact_loans"""

print('starting transformation of loans')

pd_loans_copy = pd_loans[['LoanNumber', 'DateApproved', 'InitialApprovalAmount', 'NAICSCode', 'ForgivenessAmount', 'ForgivenessDate', 'JobsReported', 'BorrowerID', 'OriginatingLender', 'LoanStatus']].copy()

pd_loans_copy['NAICSCode'] = pd_loans_copy['NAICSCode'].astype(int)
merged_df = pd.merge(pd_loans_copy, pd_lender, left_on ='OriginatingLender', right_on = 'LenderName')
merged_df = merged_df.drop(['OriginatingLender', 'LenderName'], axis=1)
status_mapping = {'Not Forgiven': False, 'Forgiven': True} #using a dictionary/map function instead of calling multiple replace functions
merged_df['LoanStatus'] = merged_df['LoanStatus'].map(status_mapping)
merged_df['JobsReported'] = merged_df['JobsReported'].astype(int)

upload_file_to_s3(merged_df, 'fact_loans.csv')